"sources=", station_id,
"&referencetime=", referenceTime,
"&elements=", elements,
"&timeoffsets=", timeoffsets
)
# Issue an HTTP GET request and extract JSON data
xs <- try(fromJSON(URLencode(url),flatten=T))
# Check if the request worked, print out any errors
if (class(xs) != 'try-error') {
df <- unnest(xs$data, cols=everything())
print("Data retrieved from frost.met.no!")
} else {
print("Error: the data retrieval was not successful!")
}
df <- df[, c("referenceTime", "value")]
df$referenceTime <- as.Date(df$referenceTime)
return(as_tsibble(df, index = "referenceTime"))
}
raw_df <- get_station_data("SN17850")
ggplot(raw_df, aes(x=referenceTime, y=value)) + geom_line()
library(tidyverse)
# find gaps
gaps <- raw_df %>%
count_gaps(.full = TRUE)
#plot gaps
ggplot(gaps, aes(x=.from, y=.n)) + geom_point() + ggtitle(paste0(length(gaps$.n), " Gaps in the time series, total missing days: ", sum(gaps$.n)))
# find the earliest date in the time series such that all following data have no gaps longer than 31 days
gaps <- gaps %>%
filter(.n > 31)
raw_df <- raw_df %>%
filter(referenceTime >= max(gaps$.to))
# Create a regular time series by filling gaps in the tsibble with n/a-s.
df_full <- fill_gaps(raw_df, .full = TRUE)
library(imputeTS)
# Impute values
df <- df_full %>% na_interpolation()
sum(is.na(df$value))
# Remove every 29. of February
df <- df %>% filter(!(month(referenceTime) == 2 & day(referenceTime) == 29))
prepare_station_data <- function(raw_df) {
# find gaps
gaps <- raw_df %>%
count_gaps(.full = TRUE)
# find the earliest date in the time series such that all following data have no gaps longer than 31 days
gaps <- gaps %>%
filter(.n > 31)
if (nrow(gaps) > 0) {
raw_df <- raw_df %>%
filter(referenceTime >= max(gaps$.to))
}
# Create a regular time series by filling gaps in the tsibble with n/a-s.
df_full <- fill_gaps(raw_df, .full = TRUE)
# Impute values
df <- df_full %>% na_interpolation()
# Remove every 29. of February
df <- df %>% filter(!(month(referenceTime) == 2 & day(referenceTime) == 29))
return(df)
}
ggplot(df, aes(x=referenceTime, y=value)) + geom_line()
ggplot() + geom_density(raw_df, mapping = aes(x=value, color="blue"), color="blue") + geom_density(df, mapping = aes(x=value, colour = "red"), color="red") + labs(title="Density plot of original(Blue) and imputed(Red) data", x="Temperature", y="Density")
ts_df <- ts(df$value, start=c(year(min(df$referenceTime)), month(min(df$referenceTime)), day(min(df$referenceTime))), frequency=365)
plot(ts_df)
library(forecast)
library(ggfortify)
autoplot(Acf(ts_df, lag.max=365*5.5))
autoplot(Acf(ts_df, lag.max=28))
# Select some days distributed throughout the year
for (month in 1:12){
day <- 1
df_day <- df %>% filter(month(referenceTime) == month & day(referenceTime) == day)
p <- ggplot(df_day, aes(x=year(referenceTime), y=value)) + geom_point() + geom_abline() + ggtitle(paste0("Temperature in ", month.name[month]))
print(p)
}
stl_as <- stl(ts_df, s.window="periodic", t.window = 11*365, l.window = 365,  robust=TRUE)
autoplot(stl_as)
autoplot(Acf(stl_as$time.series[,3], lag.max=365*5.5))
# Akershus
as_df <- df
sarpsborg_df <- get_station_data("SN3190")
sarpsborg_df <- prepare_station_data(sarpsborg_df)
gardemoen_df <- get_station_data("SN4780")
gardemoen_df <- prepare_station_data(gardemoen_df)
# Vestlandet
bergen_df <- get_station_data("SN50540")
bergen_df <- prepare_station_data(bergen_df)
haugesund_df <- get_station_data("SN47260")
haugesund_df <- prepare_station_data(haugesund_df)
sola_df <- get_station_data("SN44560")
sola_df <- prepare_station_data(sola_df)
# Nordnorge
kautokeino_df <- get_station_data("SN93700")
kautokeino_df <- prepare_station_data(kautokeino_df)
alta_df <- get_station_data("SN93140")
alta_df <- prepare_station_data(alta_df)
kirkenes_df <- get_station_data("	SN99370")
kirkenes_df <- prepare_station_data(kirkenes_df)
sprintf("First date at as is %s", min(as_df$referenceTime))
sprintf("First date at sarpsborg is %s", min(sarpsborg_df$referenceTime))
sprintf("First date at gardemoen is %s", min(gardemoen_df$referenceTime))
print("")
sprintf("First date at bergen is %s", min(bergen_df$referenceTime))
sprintf("First date at haugesund is %s", min(haugesund_df$referenceTime))
sprintf("First date at sola is %s", min(sola_df$referenceTime))
print("")
sprintf("First date at kautokeino is %s", min(kautokeino_df$referenceTime))
sprintf("First date at alta is %s", min(alta_df$referenceTime))
sprintf("First date at kirkenes is %s", min(kirkenes_df$referenceTime))
min_time <- max(min(as_df$referenceTime), min(sarpsborg_df$referenceTime), min(gardemoen_df$referenceTime), min(bergen_df$referenceTime), min(haugesund_df$referenceTime), min(sola_df$referenceTime), min(kautokeino_df$referenceTime), min(alta_df$referenceTime), min(kirkenes_df$referenceTime))
as <- as_df %>% filter(referenceTime >= min_time)
sarpsborg <- sarpsborg_df %>% filter(referenceTime >= min_time)
gardemoen <- gardemoen_df %>% filter(referenceTime >= min_time)
bergen <- bergen_df %>% filter(referenceTime >= min_time)
haugesund <- haugesund_df %>% filter(referenceTime >= min_time)
sola <- sola_df %>% filter(referenceTime >= min_time)
kautokeino <- kautokeino_df %>% filter(referenceTime >= min_time)
alta <- alta_df %>% filter(referenceTime >= min_time)
kirkenes <- kirkenes_df %>% filter(referenceTime >= min_time)
# get all into one timeseries
multivariate_tsibble <- tsibble(as, index="referenceTime") %>%
left_join(sarpsborg, by="referenceTime") %>%
left_join(gardemoen, by="referenceTime") %>%
left_join(bergen, by="referenceTime") %>%
left_join(haugesund, by="referenceTime") %>%
left_join(sola, by="referenceTime") %>%
left_join(kautokeino, by="referenceTime") %>%
left_join(alta, by="referenceTime") %>%
left_join(kirkenes, by="referenceTime")
# rename columns
colnames(multivariate_tsibble) <- c("referenceTime", "as", "sarpsborg", "gardemoen", "bergen", "haugesund", "sola", "kautokeino", "alta", "kirkenes")
multivariate_ts <- ts(multivariate_tsibble[,2:10], start=c(year(min(multivariate_tsibble$referenceTime)), month(min(multivariate_tsibble$referenceTime)), day(min(multivariate_tsibble$referenceTime))), frequency=365)
cor(multivariate_tsibble[,2:10])
places = c("as", "sarpsborg", "gardemoen", "bergen", "haugesund", "sola", "kautokeino", "alta", "kirkenes")
for (i in 1:9){
stl_i <- stl(multivariate_ts[, i], s.window="periodic", t.window = 11*365, l.window = 365,  robust=TRUE)
p <- autoplot(stl_i, main=places[i])
print(p)
}
library(jsonlite)
.client_id <- "df073266-cbaf-4903-a3c2-4ffc75fcd21b"
.stations_url = "https://df073266-cbaf-4903-a3c2-4ffc75fcd21b@frost.met.no/sources/v0.jsonld"
raw_stations <- fromJSON(URLencode(.stations_url), flatten=TRUE)
library(dplyr)
library(tidyr)
COUNTYS = c("Vestfold")   # replace with names of "fylker" you are interested in
stations <- unnest(raw_stations$data, cols='id') |>
select(id, validFrom, country, county, municipality, name, masl, `@type`) |>
mutate(validFrom=as.Date(validFrom)) |>
filter(`@type` == "SensorSystem" & validFrom <= "1980-01-01" & country == "Norge")
gc()
?stl_as
?stl
sprintf("Akershus og Østfold")
sprintf("First date at as is %s", min(as_df$referenceTime))
temp <- tempfile()
download.file("https://archive.ics.uci.edu/static/public/360/air+quality.zip", temp)
unzip(temp, exdir = tempdir())
df <- read.table( "AirQualityUCI.csv", header=TRUE, sep=";")
library(lubridate)
df$datetime <- with(df, dmy(df$Date) + hms(df$Time))
library(tsibble)
library(dplyr)
library(lubridate)
df <- df %>%
filter(Time!="")
drops = c("Date", "Time", "X", "X.1") # Removing not needed cols. X and X.1 empty, and Date, Time stored in datetime
df <- df[ , !(names(df) %in% drops)]
df <- df %>%
filter(!(as.Date(datetime) %in% as.Date(c("2004-03-10", "2005-04-04"))))
df <- df %>%
mutate(across(where(is.character), ~ as.numeric(gsub(",", ".", .))))
tsib_df <- as_tsibble(df, index = "datetime")
print(tsib_df)
library(ggplot2)
library(dplyr)
for (col in colnames(tsib_df)) {
if (col == "datetime") {
next
}
p <- ggplot(tsib_df, aes_string(x = "datetime", y = col)) +
geom_line() +
labs(title = paste("Time Series Plot of", col),
y = col) +
theme_minimal()
print(p)
}
na_counts <- sapply(tsib_df, function(x) sum(x == -200, na.rm = TRUE))
na_counts <- na_counts[order(-na_counts)]
cat(sprintf("Amount of cols: %d \n", length(na_counts)))
cat("Amount of missing values in each column\n")
for (col in names(na_counts)) {
count <- na_counts[col]
if (count > 0) {
cat(sprintf("%-13s : %d amount\n", col, count))
}
}
library(tsibble)
library(dplyr)
library(ggfortify)
# Perform PCA
pc <- prcomp(tsib_df[, names(tsib_df)!="datetime"], scale=TRUE)
screeplot(pc, main = "Scree Plot", col = "blue", type = "lines")
autoplot(pc, loadings.label=TRUE)
autoplot(pc, x=2, y = 3, loadings.label=TRUE)
missing_df <- tsib_df[tsib_df$PT08.S1.CO. ==-200, ] # Finding the index where PT08 is returning NAN
for (feature in colnames(missing_df)){
a <- sum(missing_df[[feature]]==-200)
cat(sprintf("%-13s : %-6.0f null values \n", feature, a ))
}
missing_df <- tsib_df[tsib_df$NO2.GT. ==-200, ] # Finding the index where PT08 is returning NAN
for (feature in colnames(missing_df)){
a <- sum(missing_df[[feature]]==-200)
cat(sprintf("%-13s : %-6.0f null values \n", feature, a ))
}
tsib_df_no_NMHC <- tsib_df %>%
select(-NMHC.GT.)
# Print the size of the original data frame
original_size <- dim(tsib_df_no_NMHC)
cat("Original Size: Rows =", original_size[1], ", Columns =", original_size[2], "\n")
tsib_df_no_NMHC[tsib_df_no_NMHC == -200] <- NA
# Filter out rows with NA and store the result in a new data frame
tsib_df_no_NMHC = tsib_df_no_NMHC %>%
drop_na()
# Print the size of the filtered data frame
filtered_size <- dim(tsib_df_no_NMHC)
cat("Filtered Size: Rows =", filtered_size[1], ", Columns =", filtered_size[2], "\n")
cat("Rows removed: ", original_size[1] - filtered_size[1], "\n")
library(tsibble)
library(dplyr)
library(ggfortify)
pc_cleaned <- prcomp(tsib_df_no_NMHC[, names(tsib_df_no_NMHC)!="datetime"], scale=TRUE)
screeplot(pc_cleaned, main = "Scree Plot", col = "blue", type = "lines")
autoplot(pc_cleaned, loadings.label=TRUE)
autoplot(pc_cleaned, x=2, y = 3, loadings.label=TRUE)
autoplot(pc_cleaned, x=3, y = 4, loadings.label=TRUE)
variance_explained <- summary(pc_cleaned)$importance[2, ]
variance_explained
total_variance_pc1_pc2 <- sum(variance_explained[1:2])
total_variance_pc1_pc3 <- sum(variance_explained[1:3])
total_variance_pc1_pc4 <- sum(variance_explained[1:4])
cat("Total variance explained by PC1 -> PC2: ", round(total_variance_pc1_pc2 * 100, 2), "%\n")
cat("Total variance explained by PC1 -> PC3: ", round(total_variance_pc1_pc3 * 100, 2), "%\n")
cat("Total variance explained by PC1 -> PC4: ", round(total_variance_pc1_pc4 * 100, 2), "%\n")
reconstructed_data <- t(t(pc_cleaned$x[, 1:3] %*% t(pc_cleaned$rotation[, 1:3])) * pc_cleaned$scale + pc_cleaned$center)
reconstructed_df <- as.data.frame(reconstructed_data)
reconstructed_df$datetime <- tsib_df_no_NMHC$datetime
#Plot columns from reconstructed_df and tsib_df_no_NMHC against each other
for (col in colnames(reconstructed_df[names(reconstructed_df)!="datetime"])) {
p <- ggplot() +
geom_line(data = tsib_df_no_NMHC, aes_string(x = "datetime", y = col), color = "blue") +
geom_line(data = reconstructed_df, aes_string(x = "datetime", y = col), color = "red") +
labs(title = paste("Time Series Plot of", col),
y = col) +
theme_minimal()
print(p)}
pc_scores <- pc_cleaned$x[, 1:3] # Getting pc values for all rows.
pc_scores <- as.data.frame(pc_scores)
pc_scores$datetime <- tsib_df_no_NMHC$datetime
for (col in colnames(pc_scores[, names(pc_scores) != "datetime"])) {
p <- ggplot() +
geom_line(data = pc_scores, aes_string(x = "datetime", y = col), color = "blue") +
labs(title = paste("Principal component:", col, "for all data"),
y = col) +
theme_minimal()
print(p)}
pc_scores <- pc_cleaned$x[, 1:3] # Getting pc values for all rows.
pc_scores <- as.data.frame(pc_scores)
pc_scores$datetime <- tsib_df_no_NMHC$datetime
filtered_scores <- pc_scores %>%
filter(datetime >= "2004-03-11 00:00:00" & datetime <= "2004-03-22 00:00:00")
#Print PC1, PC2 and PC3 scores
for (col in colnames(filtered_scores[, names(filtered_scores) != "datetime"])) {
p <- ggplot() +
geom_line(data = filtered_scores, aes_string(x = "datetime", y = col), color = "blue") +
labs(title = paste("Principal component:", col, "for a 11 day interval"),
y = col) +
theme_minimal()
print(p)}
# Used https://frost.met.no/r_example.html as a reference
library(jsonlite)
library(tidyr)
#function
get_station_data <- function(station_id) {
client_id <- "df073266-cbaf-4903-a3c2-4ffc75fcd21b"
# Define andpoint and parameters
endpoint <- paste0("https://", client_id, "@frost.met.no/observations/v0.jsonld")
elements <- 'mean(air_temperature P1D)'
referenceTime <- '1950-01-01/2024-09-01'
timeoffsets <- 'PT0H'
# Build the URL to Frost
url <- paste0(
endpoint, "?",
"sources=", station_id,
"&referencetime=", referenceTime,
"&elements=", elements,
"&timeoffsets=", timeoffsets
)
# Issue an HTTP GET request and extract JSON data
xs <- try(fromJSON(URLencode(url),flatten=T))
# Check if the request worked, print out any errors
if (class(xs) != 'try-error') {
df <- unnest(xs$data, cols=everything())
print("Data retrieved from frost.met.no!")
} else {
print("Error: the data retrieval was not successful!")
}
df <- df[, c("referenceTime", "value")]
df$referenceTime <- as.Date(df$referenceTime)
return(as_tsibble(df, index = "referenceTime"))
}
raw_df <- get_station_data("SN17850")
ggplot(raw_df, aes(x=referenceTime, y=value)) + geom_line()
library(tidyverse)
# find gaps
gaps <- raw_df %>%
count_gaps(.full = TRUE)
#plot gaps
ggplot(gaps, aes(x=.from, y=.n)) + geom_point() + ggtitle(paste0(length(gaps$.n), " Gaps in the time series, total missing days: ", sum(gaps$.n)))
# find the earliest date in the time series such that all following data have no gaps longer than 31 days
gaps <- gaps %>%
filter(.n > 31)
raw_df <- raw_df %>%
filter(referenceTime >= max(gaps$.to))
# Create a regular time series by filling gaps in the tsibble with n/a-s.
df_full <- fill_gaps(raw_df, .full = TRUE)
library(imputeTS)
# Impute values
df <- df_full %>% na_interpolation()
sum(is.na(df$value))
# Remove every 29. of February
df <- df %>% filter(!(month(referenceTime) == 2 & day(referenceTime) == 29))
prepare_station_data <- function(raw_df) {
# find gaps
gaps <- raw_df %>%
count_gaps(.full = TRUE)
# find the earliest date in the time series such that all following data have no gaps longer than 31 days
gaps <- gaps %>%
filter(.n > 31)
if (nrow(gaps) > 0) {
raw_df <- raw_df %>%
filter(referenceTime >= max(gaps$.to))
}
# Create a regular time series by filling gaps in the tsibble with n/a-s.
df_full <- fill_gaps(raw_df, .full = TRUE)
# Impute values
df <- df_full %>% na_interpolation()
# Remove every 29. of February
df <- df %>% filter(!(month(referenceTime) == 2 & day(referenceTime) == 29))
return(df)
}
ggplot(df, aes(x=referenceTime, y=value)) + geom_line()
ggplot() + geom_density(raw_df, mapping = aes(x=value, color="blue"), color="blue") + geom_density(df, mapping = aes(x=value, colour = "red"), color="red") + labs(title="Density plot of original(Blue) and imputed(Red) data", x="Temperature", y="Density")
ts_df <- ts(df$value, start=c(year(min(df$referenceTime)), month(min(df$referenceTime)), day(min(df$referenceTime))), frequency=365)
plot(ts_df)
library(forecast)
library(ggfortify)
autoplot(Acf(ts_df, lag.max=365*5.5))
autoplot(Acf(ts_df, lag.max=28))
# Select some days distributed throughout the year
for (month in 1:12){
day <- 1
df_day <- df %>% filter(month(referenceTime) == month & day(referenceTime) == day)
p <- ggplot(df_day, aes(x=year(referenceTime), y=value)) + geom_point() + geom_abline() + ggtitle(paste0("Temperature in ", month.name[month]))
print(p)
}
stl_as <- stl(ts_df, s.window="periodic", t.window = 11*365, robust=TRUE)
autoplot(stl_as)
autoplot(Acf(stl_as$time.series[,3], lag.max=365*5.5))
# Akershus og Østfold
as_df <- df
sarpsborg_df <- get_station_data("SN3190")
sarpsborg_df <- prepare_station_data(sarpsborg_df)
gardemoen_df <- get_station_data("SN4780")
gardemoen_df <- prepare_station_data(gardemoen_df)
# Vestlandet
bergen_df <- get_station_data("SN50540")
bergen_df <- prepare_station_data(bergen_df)
haugesund_df <- get_station_data("SN47260")
haugesund_df <- prepare_station_data(haugesund_df)
sola_df <- get_station_data("SN44560")
sola_df <- prepare_station_data(sola_df)
# Nord-Norge
kautokeino_df <- get_station_data("SN93700")
kautokeino_df <- prepare_station_data(kautokeino_df)
alta_df <- get_station_data("SN93140")
alta_df <- prepare_station_data(alta_df)
kirkenes_df <- get_station_data("	SN99370")
kirkenes_df <- prepare_station_data(kirkenes_df)
sprintf("Akershus og Østfold")
sprintf("First date at as is %s", min(as_df$referenceTime))
sprintf("First date at sarpsborg is %s", min(sarpsborg_df$referenceTime))
sprintf("First date at gardemoen is %s", min(gardemoen_df$referenceTime), "\n")
print("Vestlandet")
sprintf("First date at bergen is %s", min(bergen_df$referenceTime))
sprintf("First date at haugesund is %s", min(haugesund_df$referenceTime))
sprintf("First date at sola is %s", min(sola_df$referenceTime), "\n")
print("Finnmark")
sprintf("First date at kautokeino is %s", min(kautokeino_df$referenceTime))
sprintf("First date at alta is %s", min(alta_df$referenceTime))
sprintf("First date at kirkenes is %s", min(kirkenes_df$referenceTime))
min_time <- max(min(as_df$referenceTime), min(sarpsborg_df$referenceTime), min(gardemoen_df$referenceTime), min(bergen_df$referenceTime), min(haugesund_df$referenceTime), min(sola_df$referenceTime), min(kautokeino_df$referenceTime), min(alta_df$referenceTime), min(kirkenes_df$referenceTime))
as <- as_df %>% filter(referenceTime >= min_time)
sarpsborg <- sarpsborg_df %>% filter(referenceTime >= min_time)
gardemoen <- gardemoen_df %>% filter(referenceTime >= min_time)
bergen <- bergen_df %>% filter(referenceTime >= min_time)
haugesund <- haugesund_df %>% filter(referenceTime >= min_time)
sola <- sola_df %>% filter(referenceTime >= min_time)
kautokeino <- kautokeino_df %>% filter(referenceTime >= min_time)
alta <- alta_df %>% filter(referenceTime >= min_time)
kirkenes <- kirkenes_df %>% filter(referenceTime >= min_time)
# get all into one timeseries
multivariate_tsibble <- tsibble(as, index="referenceTime") %>%
left_join(sarpsborg, by="referenceTime") %>%
left_join(gardemoen, by="referenceTime") %>%
left_join(bergen, by="referenceTime") %>%
left_join(haugesund, by="referenceTime") %>%
left_join(sola, by="referenceTime") %>%
left_join(kautokeino, by="referenceTime") %>%
left_join(alta, by="referenceTime") %>%
left_join(kirkenes, by="referenceTime")
# rename columns
colnames(multivariate_tsibble) <- c("referenceTime", "as", "sarpsborg", "gardemoen", "bergen", "haugesund", "sola", "kautokeino", "alta", "kirkenes")
multivariate_ts <- ts(multivariate_tsibble[,2:10], start=c(year(min(multivariate_tsibble$referenceTime)), month(min(multivariate_tsibble$referenceTime)), day(min(multivariate_tsibble$referenceTime))), frequency=365)
cor(multivariate_tsibble[,2:10])
places = c("as", "sarpsborg", "gardemoen", "bergen", "haugesund", "sola", "kautokeino", "alta", "kirkenes")
for (i in 1:9){
stl_i <- stl(multivariate_ts[, i], s.window="periodic", t.window = 11*365, l.window = 365,  robust=TRUE)
p <- autoplot(stl_i, main=places[i])
print(p)
}
library(jsonlite)
.client_id <- "df073266-cbaf-4903-a3c2-4ffc75fcd21b"
.stations_url = "https://df073266-cbaf-4903-a3c2-4ffc75fcd21b@frost.met.no/sources/v0.jsonld"
raw_stations <- fromJSON(URLencode(.stations_url), flatten=TRUE)
library(dplyr)
library(tidyr)
COUNTYS = c("Vestfold")   # replace with names of "fylker" you are interested in
stations <- unnest(raw_stations$data, cols='id') |>
select(id, validFrom, country, county, municipality, name, masl, `@type`) |>
mutate(validFrom=as.Date(validFrom)) |>
filter(`@type` == "SensorSystem" & validFrom <= "1980-01-01" & country == "Norge")
sprintf("Akershus og Østfold")
sprintf("First date at as is %s", min(as_df$referenceTime))
sprintf("First date at sarpsborg is %s", min(sarpsborg_df$referenceTime))
sprintf("First date at gardemoen is %s", min(gardemoen_df$referenceTime))
print("\nVestlandet")
sprintf("First date at bergen is %s", min(bergen_df$referenceTime))
sprintf("First date at haugesund is %s", min(haugesund_df$referenceTime))
sprintf("First date at sola is %s", min(sola_df$referenceTime))
print("\nFinnmark")
sprintf("First date at kautokeino is %s", min(kautokeino_df$referenceTime))
sprintf("First date at alta is %s", min(alta_df$referenceTime))
sprintf("First date at kirkenes is %s", min(kirkenes_df$referenceTime))
sprintf("Akershus og Østfold")
sprintf("First date at as is %s", min(as_df$referenceTime))
sprintf("First date at sarpsborg is %s", min(sarpsborg_df$referenceTime))
sprintf("First date at gardemoen is %s", min(gardemoen_df$referenceTime))
cat("\nVestlandet")
sprintf("First date at bergen is %s", min(bergen_df$referenceTime))
sprintf("First date at haugesund is %s", min(haugesund_df$referenceTime))
sprintf("First date at sola is %s", min(sola_df$referenceTime))
cat("\nFinnmark")
sprintf("First date at kautokeino is %s", min(kautokeino_df$referenceTime))
sprintf("First date at alta is %s", min(alta_df$referenceTime))
sprintf("First date at kirkenes is %s", min(kirkenes_df$referenceTime))
sprintf("Akershus og Østfold")
sprintf("First date at as is %s", min(as_df$referenceTime))
sprintf("First date at sarpsborg is %s", min(sarpsborg_df$referenceTime))
sprintf("First date at gardemoen is %s", min(gardemoen_df$referenceTime))
cat("\nVestlandet\n")
sprintf("First date at bergen is %s", min(bergen_df$referenceTime))
sprintf("First date at haugesund is %s", min(haugesund_df$referenceTime))
sprintf("First date at sola is %s", min(sola_df$referenceTime))
cat("\nFinnmark\n")
sprintf("First date at kautokeino is %s", min(kautokeino_df$referenceTime))
sprintf("First date at alta is %s", min(alta_df$referenceTime))
sprintf("First date at kirkenes is %s", min(kirkenes_df$referenceTime))
cat("Akershus og Østfold")
sprintf("First date at as is %s", min(as_df$referenceTime))
sprintf("First date at sarpsborg is %s", min(sarpsborg_df$referenceTime))
sprintf("First date at gardemoen is %s", min(gardemoen_df$referenceTime))
cat("\nVestlandet\n")
sprintf("First date at bergen is %s", min(bergen_df$referenceTime))
sprintf("First date at haugesund is %s", min(haugesund_df$referenceTime))
sprintf("First date at sola is %s", min(sola_df$referenceTime))
cat("\nFinnmark\n")
sprintf("First date at kautokeino is %s", min(kautokeino_df$referenceTime))
sprintf("First date at alta is %s", min(alta_df$referenceTime))
sprintf("First date at kirkenes is %s", min(kirkenes_df$referenceTime))
cat("Akershus og Østfold\n")
sprintf("First date at as is %s", min(as_df$referenceTime))
sprintf("First date at sarpsborg is %s", min(sarpsborg_df$referenceTime))
sprintf("First date at gardemoen is %s", min(gardemoen_df$referenceTime))
cat("\nVestlandet\n")
sprintf("First date at bergen is %s", min(bergen_df$referenceTime))
sprintf("First date at haugesund is %s", min(haugesund_df$referenceTime))
sprintf("First date at sola is %s", min(sola_df$referenceTime))
cat("\nFinnmark\n")
sprintf("First date at kautokeino is %s", min(kautokeino_df$referenceTime))
sprintf("First date at alta is %s", min(alta_df$referenceTime))
sprintf("First date at kirkenes is %s", min(kirkenes_df$referenceTime))
View(multivariate_tsibble)
pc <- prcomp(tsib_df[, names(tsib_df)!="referenceTime"], scale=TRUE)
pc <- prcomp(multivariate_ts[, names(tsib_df)!="referenceTime"], scale=TRUE)
pc <- prcomp(multivariate_ts[, names(multivariate_ts)!="referenceTime"], scale=TRUE)
pc <- prcomp(multivariate_ts[, names(multivariate_ts)!="referenceTime"], scale=TRUE)
View(multivariate_tsibble)
View(multivariate_tsibble)
View(multivariate_tsibble)
View(multivariate_ts)
pc <- prcomp(multivariate_tsibble[, names(multivariate_ts)!="referenceTime"], scale=TRUE)
pc <- prcomp(multivariate_tsibble[, names(multivariate_tsibble)!="referenceTime"], scale=TRUE)
autoplot(pc, loadings.label=TRUE)
